{
    "math_word_problem_generation": {
        "gpt-4-0613": {
            "total_num": 34,
            "prediction_error_num": 18,
            "gold_error_num": 22,
            "metrics": {
                "accuracy": 0.8823529411764706,
                "precision": 1.0,
                "recall": 0.8181818181818182,
                "f1": 0.9,
                "true_negative_rate": 0.35294117647058826,
                "false_positive_rate": 0.0,
                "false_negative_rate": 0.11764705882352941,
                "true_positive_rate": 0.5294117647058824
            }
        },
        "Llama-2-70b-chat-hf": {
            "total_num": 34,
            "prediction_error_num": 29,
            "gold_error_num": 30,
            "metrics": {
                "accuracy": 0.9705882352941176,
                "precision": 1.0,
                "recall": 0.9666666666666667,
                "f1": 0.9830508474576272,
                "true_negative_rate": 0.11764705882352941,
                "false_positive_rate": 0.0,
                "false_negative_rate": 0.029411764705882353,
                "true_positive_rate": 0.8529411764705882
            }
        }
    },
    "finegrained_fact_verification": {
        "gpt-4-0613": {
            "total_num": 35,
            "prediction_error_num": 22,
            "gold_error_num": 22,
            "metrics": {
                "accuracy": 0.9428571428571428,
                "precision": 0.9545454545454546,
                "recall": 0.9545454545454546,
                "f1": 0.9545454545454546,
                "true_negative_rate": 0.34285714285714286,
                "false_positive_rate": 0.02857142857142857,
                "false_negative_rate": 0.02857142857142857,
                "true_positive_rate": 0.6
            }
        },
        "Llama-2-70b-chat-hf": {
            "total_num": 35,
            "prediction_error_num": 30,
            "gold_error_num": 30,
            "metrics": {
                "accuracy": 1.0,
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "true_negative_rate": 0.14285714285714285,
                "false_positive_rate": 0.0,
                "false_negative_rate": 0.0,
                "true_positive_rate": 0.8571428571428571
            }
        }
    },
    "answerability_classification": {
        "gpt-4-0613": {
            "total_num": 34,
            "prediction_error_num": 20,
            "gold_error_num": 22,
            "metrics": {
                "accuracy": 0.8823529411764706,
                "precision": 0.95,
                "recall": 0.8636363636363636,
                "f1": 0.9047619047619048,
                "true_negative_rate": 0.3235294117647059,
                "false_positive_rate": 0.029411764705882353,
                "false_negative_rate": 0.08823529411764706,
                "true_positive_rate": 0.5588235294117647
            }
        },
        "Llama-2-70b-chat-hf": {
            "total_num": 34,
            "prediction_error_num": 29,
            "gold_error_num": 29,
            "metrics": {
                "accuracy": 1.0,
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "true_negative_rate": 0.14705882352941177,
                "false_positive_rate": 0.0,
                "false_negative_rate": 0.0,
                "true_positive_rate": 0.8529411764705882
            }
        }
    },
    "average": {
        "gpt-4-0613": {
            "metrics": {
                "accuracy": {
                    "average": 0.9025210084033614,
                    "stdev": 0.028521954199121246
                },
                "precision": {
                    "average": 0.9681818181818181,
                    "stdev": 0.022575249129926277
                },
                "recall": {
                    "average": 0.8787878787878789,
                    "stdev": 0.05669177858748396
                },
                "f1": {
                    "average": 0.9197691197691197,
                    "stdev": 0.024667306697964242
                },
                "true_negative_rate": {
                    "average": 0.3397759103641456,
                    "stdev": 0.012203373059267207
                },
                "false_positive_rate": {
                    "average": 0.019327731092436976,
                    "stdev": 0.01367107489289908
                },
                "false_negative_rate": {
                    "average": 0.07815126050420168,
                    "stdev": 0.03705745787863371
                },
                "true_positive_rate": {
                    "average": 0.5627450980392157,
                    "stdev": 0.02895063345143803
                }
            }
        },
        "Llama-2-70b-chat-hf": {
            "metrics": {
                "accuracy": {
                    "average": 0.9901960784313726,
                    "stdev": 0.013864838846795053
                },
                "precision": {
                    "average": 1.0,
                    "stdev": 0.0
                },
                "recall": {
                    "average": 0.9888888888888889,
                    "stdev": 0.01571348402636772
                },
                "f1": {
                    "average": 0.9943502824858758,
                    "stdev": 0.007989907132051364
                },
                "true_negative_rate": {
                    "average": 0.13585434173669467,
                    "stdev": 0.012988261198173257
                },
                "false_positive_rate": {
                    "average": 0.0,
                    "stdev": 0.0
                },
                "false_negative_rate": {
                    "average": 0.00980392156862745,
                    "stdev": 0.01386483884679505
                },
                "true_positive_rate": {
                    "average": 0.8543417366946778,
                    "stdev": 0.0019806912638278573
                }
            }
        }
    }
}